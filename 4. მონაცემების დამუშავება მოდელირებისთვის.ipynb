{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. მონაცემების დამუშავება მოდელირებისთვის"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "წინა თავში ჩვენ გავიარეთ მონაცემთა კვლევისა და ვიზუალიზაციის ტექნიკები. ვნახეთ თუ რამდენად სიღრმისეულად შეიძლება გავაანალიზოთ ერთი ცვლადიც კი. თუმცა ბევრი მნიშნველოვანი ნაბიჯი გამოვტოვეთ.\n",
    "\n",
    "რეალურ ცხოვრებაში მონაცემების კვლევისა და მოდელირებისთვის მონაცემების სწორად მომზადების პროცესი ერთმანეთზე გადაჭაჭვულია, მეტიც შეგვიძლია ვთქვათ რომ ერთი მთლიანი პროცესია. თუმცა იმისათვის რომ ყველა საჭირო ტექნიკა დავფაროთ და ხაზი გავუსვათ ორივე საკითხის მნიშნველოვნებას, გადავწყვიტეთ ისინი გაგვეყო და ორ თავად წარმოგვედგინა. \n",
    "\n",
    "შესაბამისად მოცემული თავი შესაძლოა არ იყოს ისეთივე \"ჩამთრევი\" როგორც წინა. შესაძლოა ცოტა \"მშრალადაც\" მოგეჩვენოთ, თუმცა მისი წილი გამართული მოდელის ასაგებად უაღრესად მნიშნველოვანია. მომდევნო აბზაცებში ჩვენ შევეხებით ისეთ საკითხებს როგორებიცაა: ზედმეტი ცვლადების მოშორება, მონაცემებში სიცარიელეების შევსება, განცალკვებულ ჩანაწერებთან გამკლავება, ახალი ცვლადების გამოყვანა არსებული ინფორმაციიდან, კატეგორიული ცვლადების გარდაქმნა მოდელისთვის მისაღებ ფორმაში და მთლიანი მატრიცის სკალირება.\n",
    "\n",
    "*შენიშვნა: kaggle.com-ზე მოცემული მონაცემები ხშირ შემთხვევაში უკვე \"გაწმედილია\", ჩვენ კი ყველა ზემოთ ხსენებული ტექნიკის უკეთ შესასწავლად და რეალურ გარემოსთან მეტად მისაახლოვებლად მონაცემებს განზრახ დავაზიანებთ. აღნიშნული პროცესი ცდება წიგნის ფარგლებს, ამიტომ ჩვენ პირდაპირ \"დაზიანებულ\" მონაცემებს მოგაწვდით* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 მონაცემების გაწმენდა\n",
    "\n",
    "პირველ რიგში დავიწყოთ გაწმენდით და მოვიშოროთ ყველა არა საჭირო ცვლადი. ამ ყველაფრისთვის ჩვენთვის უკვე კარგად ნაცნოიბ ბიბლიოთეკებს გამოვიყენებთ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #მონაცემების დამუშავების ბიბლიოთეკა\n",
    "import numpy as np #მონაცემების დამუშავების ბიბლიოთეკა\n",
    "import seaborn as sns #მონაცემების ვიზუალიზაციის ბიბლიოთეკა\n",
    "import matplotlib.pyplot as plt #მონაცემების ვიზუალიზაციის ბიბლიოთეკა\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NYCTaxi_damaged.csv') #მთლიანი მონაცემების შემოტანა სამუშაო გარემოში"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>NaN</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>N/a</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421       2.0  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394       1.0  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529       2.0  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673       2.0                  NaN  2016-04-06 19:39:40   \n",
       "4  id2181028       N/a  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0              1.0        -73.982155        40.767937                NaN   \n",
       "1              1.0        -73.980415        40.738564         -73.999481   \n",
       "2              1.0        -73.979027        40.763939         -74.005333   \n",
       "3              1.0        -74.010040        40.719971         -74.012268   \n",
       "4              NaN        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                NaN            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "როგორც ვხედავთ მოცემული გვაქვს ჩანაწერის აიდის ცვლადი, რომელიც უნიკალურია ყველა ჩანაწერისთვის. მსგავსი ტიპის ცვლადები არანაირ ღირებულებას არ ქმნიან, შესაბამისად პირველ რიგში მისი წაშლით დავიწყოთ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id'], axis = 1, inplace = True)\n",
    "# .drop() ფუნქცია შლის მონაცემებს, axis არგუმენტით ვაზუსტებთ სვეტის წაშლა გვინდა თუ სტრიქონის. 1 შეესაბამება\n",
    "# სვეტს სწორად ამიტომ გავუტოლეთ axis 1-ს. ხოლო  inplace პარამეტრის (რომელიც პანდას ბევრ ფუნქციას მოყვება)\n",
    "# True-ზე გატოლებით ჩვენ ვეუბნებით რომ მითითებული ბრძანებით df-ს თავზე გადააწეროს ახალი მნიშნვლეობა. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "შემდეგი ნაბიჯია ისეთი ველების წაშლა, რომელთა დიდი ნაწილი არის შეუვსებელი. ჩვენ შემთხვევაში ასე მოვიქცევით 70%-ზე მეტი ცარიელი ველების მქონდე ცვლადებზე. თუმცა სანამ ველებს შევამოწმებთ შევსების პროცენტულობაზე, მნიშნველოვანია დარწმუნებულები ვიყოთ, რომ ცარიელი მონაცემები შესაბამის ფორმატშია ჩაწერილი. ხშირად სხვადასხვა სისტემები სხვადასხვაგვარ მნიშნველობებს ანიჭებენ სიცარიელებს: Nan, NaN, N/A, None და ა.შ. ხოლო პანდას ფუნქცია რომელიც აღნიშნულს ამოწმებს მხოლოდ გაკრვეულ ტიპებს აღიქვამს. ამიტომ პირველ რიგში დავწეროთ ფუნქცია რომელიც შეამოწმებს ჩამოთვლილთაგან რომელიმე ხო არ ხვდება მონაცემებში და თუ ხვდება მათ ჩაანაცვლებს np.nan მნიშნველობებით.\n",
    "\n",
    "*შენიშვნა: ზოგ შემთხვევაში ცარიელი მნიშვნელობა შესაძლოა შინაარსობრივად 0-ის ტოლი იყოს. მაგ. თუ მომხმარებლების ტრანზაციების ინფორმაცია გვაქვს და მნიშვნელობები ცარიელია, ეს იმას ნიშნავს რომ მათ არ განუხორციელებიათ ტრანზაქცია შესაბამისად ასეთი ველები 0-ით უნდა შევავსოთ და არც ერთ შემთხვევაში ასეთი ცვლადი არ უნდა წავშალოთ მცირედით შევსებულობის მიზეზით*\n",
    "\n",
    "სანამ ფუნქციას დავწერთ მოდით მანამდე ერთ მარტივ ხრიკს მივმართოთ რომელიც დაგვეხმარება ზემოთ აღნიშნული \"არა სწორი\" ფორმატით ჩაწერილი ცარიელი მნიშვნელობების დაიდენტიფიცირებაში, დავბეჭდოთ სვეტების ტიპები .info() ფუნქციის გამოყენებით."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1458644 entries, 0 to 1458643\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   vendor_id           1458644 non-null  object \n",
      " 1   pickup_datetime     1312567 non-null  object \n",
      " 2   dropoff_datetime    1458644 non-null  object \n",
      " 3   passenger_count     1312998 non-null  float64\n",
      " 4   pickup_longitude    1458644 non-null  float64\n",
      " 5   pickup_latitude     1458644 non-null  float64\n",
      " 6   dropoff_longitude   1312398 non-null  float64\n",
      " 7   dropoff_latitude    1312513 non-null  float64\n",
      " 8   store_and_fwd_flag  1312672 non-null  object \n",
      " 9   trip_duration       1458644 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(4)\n",
      "memory usage: 111.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "როგორც ვხედავთ ყველა ცვლადი \"ლოგიკურ\" ფორმატშია გარდა vendor_id-სა. როდესაც ცხრილის პირველი 5 ჩანაწერი დავბეჭდეთ ნათლად გამოჩნდა, რომ აღნიშნულ ცვლადშიც რიცხვითი მნიშნველობები იყო ჩაწერილი. ეს კი შეიძლება მინიშნება იყოს ზუსტადაც პანდასთვის \"მიუღებელ\" ფორმატში ჩაწერილი ცარიელი მნიშვნელობების. მოდით ჯერ შევამოწმოთ ცარიელი მნიშვნელობები, შევამოწმოთ გვაქვს თუ არა vendor_id-ის ცვლადში, შემდგომ დავწეროთ ფუნქცია რომელიც ყველა არასასრუველ ფორმატს გარდაქმნის ჩვენთვის სასურველში, გადავატაროთ მონაცმებს და შევამოწმოთ შეგვეცვლება თუ არა რაიმე."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id              0.000000\n",
       "pickup_datetime       10.014575\n",
       "dropoff_datetime       0.000000\n",
       "passenger_count        9.985027\n",
       "pickup_longitude       0.000000\n",
       "pickup_latitude        0.000000\n",
       "dropoff_longitude     10.026161\n",
       "dropoff_latitude      10.018277\n",
       "store_and_fwd_flag    10.007377\n",
       "trip_duration          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean() * 100\n",
    "\n",
    "# როგორც ვხედავთ გარკვეული მონაცემები 10%-ის ფარგლებში არის ცარიელი, თუმცა \"ეჭვმიტანილი\" vendor_id ცვლადი\n",
    "# ერთი შეხედვით სრულად შევსებულია"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_most_pop_miss(X):\n",
    "    dict_nans = dict.fromkeys(['?', 'n/a', 'nan', 'NaN', ',', 'N/a', 'N/A', 'none', '*', '.'], np.nan)\n",
    "    objects_list = X.select_dtypes(include = ['object']).columns\n",
    "    X[objects_list] = X[objects_list].replace(dict_nans)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = replace_most_pop_miss(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_id              9.960004\n",
       "pickup_datetime       10.014575\n",
       "dropoff_datetime       0.000000\n",
       "passenger_count        9.985027\n",
       "pickup_longitude       0.000000\n",
       "pickup_latitude        0.000000\n",
       "dropoff_longitude     10.026161\n",
       "dropoff_latitude      10.018277\n",
       "store_and_fwd_flag    10.007377\n",
       "trip_duration          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ზემოთ მოყვანილი ცხრილიდან ჩანს, რომ არც ერთი ცვლადი არ იმსახურებს წაშლას (70% პრინციპის გამოყენებით) თუმცა ჩვენ მაინც დავწეროთ ფუნქცია რომელიც წაშლის ყველა მსგავს ცვლადს."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**დაწერეთ ფუნქცია რომელიც არგუმენტად მიიღებს დატა ფრეიმს, მიღებულ დატაფრემიდან წაშლის ყველა ისეთ ცვლადს რომელის ცარიელი მნიშნველობებიც აღებატება 70% და დააბრუნებს გაწმენდილ დატაფრეიმს**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dropping_70_nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "მონაცემების გაწმენდის შემდგომი ეტაპია, დუბლირებული ჩანაწერების წაშლა. შინაარსობრივადაც, რომ დავუკვირდეთ ძალიან უცნაური იქნება რეგრესიულ ამოცანაში ორი ერთმანეთის იდენტური სტრიქონი. ჩვენი მონაცემების მაგალითზე რომ ვთქვათ: ორი ტაქსი რომლებიც წარმოადგენდნენ ერთსა და იმავე კომპანიას. მგზავრები აიყვანეს იდენტურ დროს, დანიშნულების ადგილას მივიდნენ იდენტურ დროს, იდენტური მგზავრების რაოდენობით, ინდენტური გეოგრაფიულ ლოკაციებზე. რა თქმა უნდა ასეთი ჩანაწერი თითქმის დარწმუნებით შეგვიძლია ვთქვათ, რომ გაფუჭებულია და მოდელსაც აბნევს. შესაბამისად მსგავსი ტიპის ჩანაწერები უნდა წავშალოთ, როგორც რეგრესიულ ისე კლასიფიკაციის ამოცანებში. განსხვავება მხოლოდ ერთია: რეგრესიული ამოცანის დროს დუბლირებას ვუწოდებთ საპროგნოზო ცვლადის ჩათვლით, ხოლო კლასიფიკაციაში მის გარეშე. ლოგიკურიცაა, თუ ვპორგნოზირებთ ორ-კლასიან ცვლადს, რომ გამეორდეს უცნაურია არაფერია. ხოლო უწყვეტი ცვლადის ზუსტი განმეორება უკვე ეჭვქვეშ აყენებს მონაცემის სისწორეს.\n",
    "\n",
    "ამ ოპერაციისთვის პანდას გააჩნია შესაბამისი ფუქნცია რომელიც თავიდან გვაცილებს დუბლირებულ ჩანაწერებს. იმისათვის, რომ დავინახოთ იყო თუ არა ჩვენს მონაცემებში მსგავსი ჩანაწერები, აღნიშნული ფუქნციის გამოყენებამდე და გამოყენების შემდგომ დავბეჭდოთ ცხრილის ჩანაწერების რაოდენობა."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458644"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458644"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "საბედნიეროდ ჩვენს ჩანაწერებში დუბლირებული მონაცემები არ აღმოჩნდა, თუმცა აღნიშნული ნაბიჯის შესრულება აუცილებელია. წინააღმდეგ შემთხვევასი შესაძლოა უარყოფითი გავლენა მოვახდინოთ მოდელის სიზუსტეზე.\n",
    "\n",
    "\n",
    "წინა თავში როდესაც მონაცემებს ვიკვლევდით, განცალკევებულმა მონაცემება საკმაოდ შეგვიშალა ხელი. ზოგადად როგორც ანალიკურ ისე მოდელირების ნაწილში განცალკევებული მონაცემებთან გამკლავება უაღრესად მნიშნველოვანია. უკეთ რომ ავხსნათ, ნებისმიერი მოდელი უნდა იყოს განზოგადებული და ჩვენი მოლოდინიც უნდა იყოს რომ \"უმეტეს\" შემთხვევაში მუშაობდეს. შესაბამისად თუ განცალკევებულ (გამონაკლის) მონაცემებს არ შევიყვანთ მოდელში და მასაც არ ეცოდინება მსგავსი ტიპის ჩანაწერებზე კარგად პროგნოზირება, პრობლემა არ არის. მაგრამ პირიქით პრობლემა, რადგან როგორც აღვნიშნედ ისინი ხელს გვიშლიან როგორც ანალიზის ფაზაში დასკვნების გაკეთებაში, ისე \"აბნევენ\" ბევრ ალგორითს. ამიტომაც უმჯობესია ყოველთვის გამოვიკვლიოთ და თუ საჭიროა წავშალოთ კიდეც.\n",
    "\n",
    "ერთი მარტივი და ზოგადი წესი შეგვიძლია დავნერგოთ, რომელსაც ხშირად მიმართავენ. დავიტოვოთ მხოლოდ ისეთი მონაცემები რომელიც ცვლადის 25-ე და 75- კვანტილებიდან 1.5-ჯერ კვარტილთაშორისი სიდიდით არიან დაშორებუელბი. ასეთ მეთოდს შესაძლოა ბევრი ისეთი ჩანაწერიც \"ემსხვერპლოს\" რომელიც დიდად არ იმსახურებდა წაშლას, თუმცა მეორეს მხრივ დაზღვეულები ვართ, რომ განცალკევებული მონაცემები ხელს არ შეგვიშლიან. ჩვენც დავწეროთ ფუნქცია რომელიც ცხრილის და სვეტის გადაცემებით, გაფილტრულ ცხრილს დააბრუნებს."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "სანამ ცვლადს საკუთარ \"გაფილტრულ\" თავს გადავაწერთ, მანამდე შევამოწმოთ თუ რამდენ მონაცემს წაშლის ჩვენი ფუქნცია თითოეული ცვლადისთვის. თავდაპირველად შევქმნათ ასეთი ცვლადების ჩამონათვალი."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_to_chekc = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "                     'dropoff_latitude', 'trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_longitude 0.942191514859006\n",
      "pickup_latitude 0.9638410743128549\n",
      "dropoff_longitude 0.8515827028390752\n",
      "dropoff_latitude 0.8553375600900562\n",
      "trip_duration 0.9490458261234407\n"
     ]
    }
   ],
   "source": [
    "for i in outliers_to_chekc:\n",
    "    print(i, drop_outliers(df, i).shape[0] / df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "როგორც ხედავთ დანიშნულების ადგილის გრძედსა და განედზე თითქმის 15% მოექცა ჩვენი წესით განსაზღვრუ განცალკევებულ მონაცემებად. თუმცა იქედან გამომდინარე, რომ ზოგადად გრძედი და განედი სპეციფიური მონაცემებია და მას სტარტული გზა ნაკლებად ერგება, სწორი იქნება თუ მთლიან ცხრილს მხოლოდ მგზავრობის ხანგრძლივობით გავფილტრავთ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9490458261234407"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# დასაწრმუნებლად შევამოწმოთ რამდენად სწორად იმუშავა ფუნქციამ\n",
    "# df_1 = drop_outliers(df, 'trip_duration')\n",
    "# df_1.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ამ ეტაპზე შეგვიძლია ვთქვათ, რომ განცალკევებულ მონაცემებს გავუმკლავდით. თუმცა უაღრესად მნიშნველოვანია ამ პროცესში ცვლადებისა და ამოცანის შინაარსის გათვლისწინება და ვნახეთ რომ არ შეიძლება რაიმე ერთი ზოგადი წესი მოვარგოთ ყველაფერს.\n",
    "\n",
    "ჩვენ რამდენიმე აბზაცის წინ წავლშალეთ დუბლირებული მონაცემები. თუმცა ეს დუბლირება მოიცავთა მთლიანი სტრიქონის ერთნაირობას. არაფერი გვითქვამს იმ შემთხვევაზე, როდესაც ცვლადი არის კონსტანტა. რა თქმა უნდა ასეთი ცვლადები არაფრის მომცემია და ისინიც უნდა წავშალოთ მონაცემების დამუშავების პროცესში. მართალია ჩვენს ამოცანაში ასეთი ცვლადი არ არის, თუმცა მაინც შემოგთავაზებთ ფუნქციას რომელიც მომავალში აღნიშნული პრობლემის გადაჭრაში დაგეხმარებათ. \n",
    "\n",
    "მსგავსი შემთხვევის შესამოწმებლად შეგვიძლია ვარიაცია გამოვიყენოთ. თუ ცვლადს აქვს ნულოვანი ვარიაცი ეს ავტომატურად ნიშნავს, რომ მისი ყველა ჩანაწერი არის ერთმანეთის ტოლი. შესაბამისად როგორც აღვნიშნეთ ასეთი ცვლადები არანაირი ინფორმაციის მატარებლები არ არიან მოდელისთვის. მაგალითად ჩვენ მონაცემებში რომ გვქონდეს ცვლადი \"ქვეყანა\", თითოეული მისი ჩანაწერი იქნებოდა აშშ, რაც მოდელს არანაირ ღირებულებას არ შესძენდა.\n",
    "\n",
    "ზოგადად მანქანური დასწავლის ვერც ერთი ალგორითმი ვერ აღიქვამს ტექსტურ ინფორმაციას. შესაბამისად ასეთი ცვლადების არსებობის შემთხვევაში საჭიროა მათი კოდირება მანქანისთვის გასაგებ ენაზე. მაგალითად თუ გვაქვს ცვლადი სადაც წერია ძაღლი ან კატა, ეს უნდა გადავთარგმნოთ 1 ან 0, ან კიდევ რაიმე სხვა რიცხვით ფორმაში. ასევე შეუძლებელია ტექსტურ ცვლადზე ვარიაციის დათვლაც. სწორად ამიტომ ჯერ ჩვენ მოვახდენთ ცვლადების კოდირებას და შემდგომ შევამოწმებთ ნულოვან ვარიაციაზე."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['store_and_fwd_flag'])\n",
    "df['store_and_fwd_flag'] = le.transform(df['store_and_fwd_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_variance_columns(df):\n",
    "    var_thres = VarianceThreshold(threshold = 0)\n",
    "    df1 = df._get_numeric_data()\n",
    "    var_thres.fit(df1)\n",
    "    constant_columns = [column for column in df1.columns\n",
    "                    if column not in df1.columns[var_thres.get_support()]]\n",
    "    df = df.drop(constant_columns, axis = 1)\n",
    "    return constant_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_variance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "მონაცემების წმენდის შემდეგი ეტაპია მაღალ-კორელირებული ცვლადების წაშლა. ამ პროცესს ორი ძირითადი მიზეზი აქვს: 1) მაპროგნოზირებელი ცვლადების მაღალი კორელაცია მიუთითებს იმაზე, რომ ისინი ერთი ინფორმაციის მატარებლები არიან. შესაბამისად გამოთვლითი რესურსის მეტად ეფექტურად გამოსაყენებლად ყოველთვის ჯობია მოვიცილოთ ყველა ისეთი ცვლადი, რაც არ არის ღირებული ინფორმაციის მატარებელი. 2) საპროგნოზო ცვლადთან მაღალი კორელაცია ნიშნავს, რომ ფაქტიურად ამხსნელი ცვლადი გვაქვს მონაცემებში. ეს შეიძლება ნიშნავდეს იმას, რომ იმდენად მარტივი დამოკიდებულება საპროგნოზო და მაპროგნოზირებელ ცვლადებს შორის, რომ საერთოდ არ არის მოდელი საჭირო. ან მაპროგნოზირებელ ცვლადებში გაპარული გვაქვს ისეთი \"მომავლის\" ინფომაცია რაც რეალურ ცხოვრებაში აღარ გვექნება (ჩვენი ამოცანის მაგალითზე ასეთი ცვლადი იქნებოდა მგზავრობის დასრულება), შესაბამისად ასეთი ცვლადები ყოველთვის უნდა ამოვაგდოთ მოდელირების პროცესიდან.\n",
    "\n",
    "აღნიშნული ოპერაციის ჩასატარებლად ჩვენ შემდეგ წესს გთავაზობთ: მაპროგოზნირებელი ცვლადებისთვის გამოვიყენოთ 0.85 კორელაციის ზღვრად, ხოლო საპრონოზოსთან 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# კორელაციური მატრიცის შექმნა აბსოლუტური მნიშვენლობებით\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features \n",
    "df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_corr (df, column_list, threshold):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    df = df.drop(to_drop, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "იმისათვის რომ წარმოდგენა შევიქმნათ თუ რა ცვლადებს წაშლის ჩვენი შექმნილი ფუქნცია, ჯერ დავითვალოთ კორელაციული მატრიცა და თვალით ვნახოთ რა ხდება ჩვენს ცხრილში."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>-0.005254</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>-0.003323</td>\n",
       "      <td>0.008344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_longitude</th>\n",
       "      <td>0.002154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022568</td>\n",
       "      <td>0.796916</td>\n",
       "      <td>0.103825</td>\n",
       "      <td>0.026542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_latitude</th>\n",
       "      <td>-0.005254</td>\n",
       "      <td>0.022568</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119777</td>\n",
       "      <td>0.499523</td>\n",
       "      <td>-0.029204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.796916</td>\n",
       "      <td>0.119777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.121336</td>\n",
       "      <td>0.016737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <td>-0.003323</td>\n",
       "      <td>0.103825</td>\n",
       "      <td>0.499523</td>\n",
       "      <td>0.121336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_duration</th>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.026542</td>\n",
       "      <td>-0.029204</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   passenger_count  pickup_longitude  pickup_latitude  \\\n",
       "passenger_count           1.000000          0.002154        -0.005254   \n",
       "pickup_longitude          0.002154          1.000000         0.022568   \n",
       "pickup_latitude          -0.005254          0.022568         1.000000   \n",
       "dropoff_longitude        -0.000557          0.796916         0.119777   \n",
       "dropoff_latitude         -0.003323          0.103825         0.499523   \n",
       "trip_duration             0.008344          0.026542        -0.029204   \n",
       "\n",
       "                   dropoff_longitude  dropoff_latitude  trip_duration  \n",
       "passenger_count            -0.000557         -0.003323       0.008344  \n",
       "pickup_longitude            0.796916          0.103825       0.026542  \n",
       "pickup_latitude             0.119777          0.499523      -0.029204  \n",
       "dropoff_longitude           1.000000          0.121336       0.016737  \n",
       "dropoff_latitude            0.121336          1.000000      -0.019741  \n",
       "trip_duration               0.016737         -0.019741       1.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "როგორც ხედავთ ყველაზე მაღალი კორელაცია არის 0.79 რაც მგზავრობის აყვანისა და დასრულების გრძედებს შორისაა. რა თქმა უნდა ეს არაფრის მთქმელია, რადგა საქმე გეოლოკაციურ ცვლადებთან გვაქვს და ისევე როგორც განცალკევებული მონაცემების შემთხვევაში გამოვტოვეთ ასეთი ტიპი ცვლადები, კორელაციური დამოშავების შემთხვევაშიც იგივე უნდა ვქნათ. ჩვენ მიერ შემოთავაზებული კორელაციური ზღვრები, გავლენას არ მოახდენს. შესაბამისად ჩვენი ამოცანისთვის ეს ნაბიჯი შეგვიძლია გამოვტოვოთ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "მგზავრობის ხანგრძლივობის ხანგრძლიოვბის 2 საათზე მოჭრამ, არა მხოლოდ მკვეთრად გამოხატული უცნაურობები გააქრო, არამედ ზოგადად მთელი გრაფიკი უფრო დააგლუვა. \n",
    "\n",
    "ზოგადად მსგავსი ტიპის ოპერაციას განცალკევებული მონაცემების გადაყრას ვუწოდებთ ხოლმე. ის ერთ-ერთი უმნიშნველოვანესი ნაწილია, როდესაც გვსურს როგორც მონაცემების სწორად გაანალიზება ისე მოდელირება. თუმცა ამ პროცესის გარდა კიდევ ბევრი ნაბიჯია გასავლელი იქამდე სანამ მოდელის პირველ დამუშავებულ ვერსიას გავუშვებთ. ამ პროცესების ერთობლიობას მონაცემების მომზადება ეწოდება და იდეურად ის მონაცემების კვლევითი ანალიზის განუყოველი ნაწილია. თუმცა ჩვენ გვურს ეს შემდგომ თავში გავიაროთ, რათა უფრო მეტ დეტალებზე კონცენტრირება შევძლოთ და სიახლეებიც ნაბიჯ-ნაბიჯ ავითვისოთ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
